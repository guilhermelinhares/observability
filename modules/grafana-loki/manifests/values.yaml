# global:
  # -- configures cluster domain ("cluster.local" by default)
  # clusterDomain: "cluster.local"
  # -- configures DNS service name
  # -- host not found in resolver "kube-dns.kube-system.svc.cluster.local." in /etc/nginx/nginx.conf:33
  # -- nginx: [emerg] host not found in resolver "kube-dns.kube-system.svc.cluster.local." in /etc/nginx/nginx.conf:33
  # dnsService: "rke2-coredns-rke2-coredns"
  # -- configures DNS service namespace
  # dnsNamespace: "kube-system"

# -- Deployment mode lets you specify how to deploy Loki.
# There are 3 options:
# - SingleBinary: Loki is deployed as a single binary, useful for small installs typically without HA, up to a few tens of GB/day.
# - SimpleScalable: Loki is deployed as 3 targets: read, write, and backend. Useful for medium installs easier to manage than distributed, up to a about 1TB/day.
# - Distributed: Loki is deployed as individual microservices. The most complicated but most capable, useful for large installs, typically over 1TB/day.
# There are also 2 additional modes used for migrating between deployment modes:
# - SingleBinary<->SimpleScalable: Migrate from SingleBinary to SimpleScalable (or vice versa)
# - SimpleScalable<->Distributed: Migrate from SimpleScalable to Distributed (or vice versa)
# Note: SimpleScalable and Distributed REQUIRE the use of object storage.
deploymentMode: SimpleScalable

loki:
  # Should authentication be enabled
  # Enables authentication through the X-Scope-OrgID header, which must be present
  # if true. If false, the OrgID will always be set to 'fake'.
  # CLI flag: -auth.enabled
  auth_enabled: false
  # -- Limits config
  limits_config:
    reject_old_samples: true
    reject_old_samples_max_age: 168h #hours-> months 4380h == 6 months || 8760h == 12 months #fix -> final error sending batch" status=400 tenant= error="server returned HTTP status 400 Bad Request (400): entry with timestamp
    max_cache_freshness_per_query: 10m
    split_queries_by_interval: 15m
    query_timeout: 300s
    volume_enabled: true
    ingestion_rate_mb: 1024 # Fix ingestion rate limit exceeded error="server returned HTTP status 429 Too Many Requests (429)
    ingestion_burst_size_mb: 1024 # Fix ingestion rate limit exceeded  error="server returned HTTP status 429 Too Many Requests (429)
    max_query_lookback: 672h # 28 days
    retention_period: 672h   # 28 days
  # -- Storage config. Providing this will automatically populate all necessary storage configs in the templated config.
  storage:
    # Loki requires a bucket for chunks and the ruler. GEL requires a third bucket for the admin API.
    # Please provide these values if you are using object storage.
    # bucketNames:
    #   chunks: FIXME
    #   ruler: FIXME
    #   admin: FIXME
    type: s3
    s3:
      s3: null
      endpoint: null
      region: null
      secretAccessKey: null
      accessKeyId: null
      signatureVersion: null
      s3ForcePathStyle: false
      insecure: false
      http_config: {}
      # -- Check https://grafana.com/docs/loki/latest/configure/#s3_storage_config for more info on how to provide a backoff_config
      backoff_config: {}
      disable_dualstack: false
    filesystem:
      chunks_directory: /var/loki/chunks
      rules_directory: /var/loki/rules
      admin_api_directory: /var/loki/admin
  # -- Configure memcached as an external cache for chunk and results cache. Disabled by default
  # must enable and specify a host for each cache you would like to use.
  memcached:
    chunk_cache:
      enabled: false
      host: ""
      service: "memcached-client"
      batch_size: 256
      parallelism: 10
    results_cache:
      enabled: false
      host: ""
      service: "memcached-client"
      timeout: "500ms"
      default_validity: "12h"
  # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas
  schemaConfig:
    configs:
      - from: 2024-04-01
        index:
          period: 24h
          prefix: index_
        object_store: s3
        schema: v13
        store: tsdb
  #https://grafana.com/docs/loki/latest/configure/storage/#index-storage
  #https://grafana.com/docs/loki/latest/configure/storage/#on-premise-deployment-minio-single-store
  #https://grafana.com/docs/loki/latest/operations/storage/tsdb/#example-configuration
  # storage_config:
  #   boltdb_shipper:
  #     index_gateway_client:
  #       server_address: '{{ include "loki.indexGatewayAddress" . }}'
  #   hedging:
  #     at: 250ms
  #     max_per_second: 20
  #     up_to: 3
  #   tsdb_shipper:
  #     active_index_directory: /data/tsdb-index
  #     cache_location: /data/tsdb-cache
  #     index_gateway_client:
  #       server_address: '{{ include "loki.indexGatewayAddress" . }}'
  # --  Optional compactor configuration
  compactor:
    working_directory: /data/retention
    # compaction_interval: 10m
    retention_enabled: true
    # retention_delete_delay: 2h
    # retention_delete_worker_count: 150
    delete_request_store: s3
  # query_scheduler:
  #   # the TSDB index dispatches many more, but each individually smaller, requests. 
  #   # We increase the pending request queue sizes to compensate.
  #   max_outstanding_requests_per_tenant: 32768
  # --  Optional querier configuration
  querier:
    # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
    max_concurrent: 4
  # --  Optional ingester configuration
  ingester:
    chunk_encoding: snappy
  # -- Optional distributor configuration
  distributor: {}
  # -- Enable tracing
  tracing:
    enabled: true
  
######################################################################################################################
#
# Gateway and Ingress
#
# By default this chart will deploy a Nginx container to act as a gateway which handles routing of traffic
# and can also do auth.
#
# If you would prefer you can optionally disable this and enable using k8s ingress to do the incoming routing.
#
######################################################################################################################

# Configuration for the gateway
gateway:
  # -- Specifies whether the gateway should be enabled
  enabled: true
  # -- Number of replicas for the gateway
  replicas: 1
  autoscaling:
    # -- Enable autoscaling for the gateway
    enabled: true
    # -- Minimum autoscaling replicas for the gateway
    minReplicas: 1
    # -- Maximum autoscaling replicas for the gateway
    maxReplicas: 3
    # -- Target CPU utilisation percentage for the gateway
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the gateway
    targetMemoryUtilizationPercentage: 60
  # -- Environment variables to add to the write pods
  extraEnv:
    - name: TZ
      value: America/Fortaleza
  # -- Node selector for write pods
  # Gateway ingress configuration
  # ingress:
  #   # -- Specifies whether an ingress for the gateway should be created
  #   enabled: false
  #   # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
  #   ingressClassName: "nginx"
  #   # -- Hosts configuration for the gateway ingress, passed through the `tpl` function to allow templating
  #   hosts:
  #     - host: ""
  #       paths:
  #         - path: /
  #           # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
  #           pathType: Prefix
  #   # # -- TLS configuration for the gateway ingress. Hosts passed through the `tpl` function to allow templating
  #   tls:
  #     - secretName: crt-key-tls
  #       hosts:
          # - ""

######################################################################################################################
#
# Simple Scalable Deployment (SSD) Mode
#
# For small to medium size Loki deployments up to around 1 TB/day, this is the default mode for this helm chart
#
######################################################################################################################
# Configuration for the write pod(s)
write:
  # -- Number of replicas for the write
  replicas: 3
  autoscaling:
    # -- Enable autoscaling for the write.
    enabled: false
    # -- Minimum autoscaling replicas for the write.
    minReplicas: 2
    # -- Maximum autoscaling replicas for the write.
    maxReplicas: 6
    # -- Target CPU utilisation percentage for the write.
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilization percentage for the write.
    targetMemoryUtilizationPercentage: 60
  # -- Environment variables to add to the write pods
  extraEnv:
    - name: TZ
      value: America/Fortaleza
  # -- Node selector for write pods
  persistence:
    # -- Enable volume claims in pod spec
    volumeClaimsEnabled: true
    # -- Parameters used for the `data` volume when volumeClaimEnabled if false
    dataVolumeParameters:
      emptyDir: {}
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: false
    # -- Size of persistent disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Selector for persistent disk
    selector: null
    # -- Annotations for volume claim
    annotations: {}
# --  Configuration for the read pod(s)
read:
  # -- Number of replicas for the read
  replicas: 3
  autoscaling:
    # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`
    enabled: false
    # -- Minimum autoscaling replicas for the read
    minReplicas: 2
    # -- Maximum autoscaling replicas for the read
    maxReplicas: 6
    # -- Target CPU utilisation percentage for the read
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the read
    targetMemoryUtilizationPercentage: 60
  # -- Environment variables to add to the write pods
  extraEnv:
    - name: TZ
      value: America/Fortaleza
  persistence:
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: true
    # -- Size of persistent disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Selector for persistent disk
    selector: null
    # -- Annotations for volume claim
    annotations: {}
# --  Configuration for the backend pod(s)
backend:
  # -- Number of replicas for the backend
  replicas: 3
  autoscaling:
    # -- Enable autoscaling for the backend.
    enabled: false
    # -- Minimum autoscaling replicas for the backend.
    minReplicas: 3
    # -- Maximum autoscaling replicas for the backend.
    maxReplicas: 6
    # -- Target CPU utilization percentage for the backend.
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilization percentage for the backend.
    targetMemoryUtilizationPercentage: 60
  # -- Environment variables to add to the write pods
  extraEnv:
    - name: TZ
      value: America/Fortaleza
  # -- Volume mounts to add to the backend pods
  extraVolumeMounts:
    - name: data
      mountPath: /data/retention
  # -- Volumes to add to the backend pods
  extraVolumes:
    - name: data
      emptyDir: {}
  persistence:
    # -- Enable volume claims in pod spec
    volumeClaimsEnabled: true
    # -- Parameters used for the `data` volume when volumeClaimEnabled if false
    dataVolumeParameters:
      emptyDir: {}
    # -- Enable StatefulSetAutoDeletePVC feature
    enableStatefulSetAutoDeletePVC: true
    # -- Size of persistent disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClass: null
    # -- Selector for persistent disk
    selector: null
    # -- Annotations for volume claim
    annotations: {}
# -- Configuration for the minio subchart
minio:
  enabled: true
  replicas: 1
  # Minio requires 2 to 16 drives for erasure code (drivesPerNode * replicas)
  # https://docs.min.io/docs/minio-erasure-code-quickstart-guide
  # Since we only have 1 replica, that means 2 drives must be used.
  drivesPerNode: 1
  buckets:
  - name: chunks
    policy: none
    purge: false
  - name: ruler
    policy: none
    purge: false
  - name: admin
    policy: none
    purge: false
  persistence:
    size: 5Gi
    annotations: {}